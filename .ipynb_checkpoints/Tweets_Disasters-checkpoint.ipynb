{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66b7b516",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-afd8e534c5ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#Libraries for plotting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;31m#Modules for plotting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "#Import scikit-learn dataset library\n",
    "import pandas as pd\n",
    "#Text processing libraries\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords') \n",
    "#sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "#Libraries for plotting\n",
    "import seaborn as sns\n",
    "#Modules for plotting\n",
    "from matplotlib import pyplot as plt\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point , Polygon\n",
    "import descartes\n",
    "#Import Nominatim for transform city names\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "df = pd.read_csv('tweets.csv').set_index('id')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a49897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying a first round of text cleaning techniques to the text column\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "# Applying the cleaning function\n",
    "df['text'] = df['text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: deEmojify(x))\n",
    "# I'm going to comment on this line that I think is the one who return an error\n",
    "#df.convert_dtypes['location'] = df['location'].apply(lambda x: deEmojify(x))\n",
    "\n",
    "# Let's take a look at the updated text\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e52ea",
   "metadata": {},
   "source": [
    "## Cleaning the 'Keyword' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b486344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we're going to check this column\n",
    "df.keyword.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70454bb",
   "metadata": {},
   "source": [
    "As we can see there are 219 unique values in this column, let's see what are those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adadc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the unique values in the 'keyword' column\n",
    "keywords = df['keyword'].unique()\n",
    "\n",
    "# sort them alphabetically and then take a closer look\n",
    "keywords.sort()\n",
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bea171c",
   "metadata": {},
   "source": [
    "As we can see the only characters that are unusual are \"%20\" ones, so we can replace them for an empty space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed87a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the \"%20\" from the registers in the column 'keyword'\n",
    "df['keyword'] = df['keyword'].apply(lambda x: x.replace(\"%20\",\" \"))\n",
    "\n",
    "#Change the dtype of the column from object to str\n",
    "df.keyword = df.keyword.astype(str)\n",
    "\n",
    "# get all the unique values in the 'keyword' column\n",
    "keywords = df['keyword'].unique()\n",
    "\n",
    "# sort them alphabetically and then take a closer look\n",
    "keywords.sort()\n",
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66161628",
   "metadata": {},
   "source": [
    "As we can see, the unique values still remain 219, but we successfully removed all the '%20' characters from our registers in the 'keyword' column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113ec1f5",
   "metadata": {},
   "source": [
    "# Transform the emoji flag to a string in the 'location' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b506c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply quotation marks to all registers in the \"location\" column (this will allowed us to treat the emoji flags later with the\n",
    "# emoji library).\n",
    "df.location = df.location.astype(str)\n",
    "df.update(df[['location']].applymap('\\'{}\\''.format))\n",
    "\n",
    "# Let's take a look at the updated text\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a0b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the emoji flags in the location column \n",
    "def emojiflag_to_text(flag):\n",
    "    '''Convert the emoji flag to a string with their name. the emoji flags are two unicode letters that composes \n",
    "    for example ðŸ‡²ðŸ‡½ -> MX, so we know that the lenght of an emoji flag is 2, as we put it in a simple quotation marks, \n",
    "    we can assume that an emoji flag has lenght equal to 4'''\n",
    "    if len(flag) == 4:\n",
    "      return emoji.demojize(flag)\n",
    "    else:\n",
    "      return flag\n",
    "     \n",
    "\n",
    "\n",
    "# Applying the transform function\n",
    "df['location'] = df['location'].apply(lambda x: emojiflag_to_text(x))\n",
    "\n",
    "# Let's take a look at the updated text\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0eb31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the simple quotes from all the registers in the \"location\" column.\n",
    "df['location'] = df['location'].apply(lambda x: x.replace(\"'\",\"\"))\n",
    "\n",
    "#Remove the \":\" from the registers that used to be an emoji flag\n",
    "df['location'] = df['location'].apply(lambda x: x.replace(\":\",\"\"))\n",
    "\n",
    "#Remove the strings nan with a true NaN\n",
    "df['location'] = df['location'].apply(lambda x: x.replace(\"nan\",\"\"))\n",
    "\n",
    "#Remove all the emojis (no emoji flags) that remains in the 'location' column\n",
    "df['location'] = df['location'].str.replace('[^\\w\\s#@/:%.,_-]', '', flags=re.UNICODE)\n",
    "\n",
    "# Let's take a look at the updated text\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe54aea",
   "metadata": {},
   "source": [
    "# Taking note of dataset issues\n",
    "For the dataset Tweets we can see that only the column called location has missing values. We only have **7,952** locations of **11,370** Tweets.\n",
    "And some emojis in text that we have to clean later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8154578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "print('\\n')\n",
    "print('Data Type')\n",
    "print('__'*12)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08746ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "nul_locations = df['location'].isna().sum()\n",
    "print(f'We have {nul_locations} missing locations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e329fd",
   "metadata": {},
   "source": [
    "In order to get the best data type to work with we can use `.convert_dtypes()` . To change the data type and have a better workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5de32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data Type Converted')\n",
    "print('__'*12)\n",
    "df.convert_dtypes().dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ea1dd9",
   "metadata": {},
   "source": [
    "## Here we have another look.\n",
    "*   219 unique keywords that describes the tweets\n",
    "*   4,504 unique locations wich need to be grouped later on\n",
    "*   11,223 Tweets (text) wich make us think if there is a kind of   duplicated message. \n",
    "*   And 2 different variables in target, 0 and 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0858ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use unique to know the different variables that conforms each column\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e184c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if there is any duplicated text\n",
    "dup_text = df['text'].duplicated().sum()\n",
    "print(f'We have {dup_text} duplicated texts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f96a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can have a description of the lenght of the text, the largest and shortest text and maybe a relation with the target number. \n",
    "df['length'] = [len(i) for i in df['text']]\n",
    "df['length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e44eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram with the distribution of the Tweets\n",
    "plt.hist(df.length)\n",
    "plt.xlabel('Length of Tweet')\n",
    "plt.ylabel('# of Tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533b2913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table with the highest and lowest lengths\n",
    "display(df[df['length']>130])\n",
    "print('\\n')\n",
    "display(df[df['length']<20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd18f78",
   "metadata": {},
   "source": [
    "# Where do the Tweets come from?\n",
    "We take a first look to the availables places that we have. We are getting the locations that occurs at least 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02278c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = df['location'].value_counts()\n",
    "location[location>=10][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e4453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to group some places, so we create a mapping dictionary to replace the locations\n",
    "mapping = {'United States':'USA',\n",
    "          'New York':'USA',\n",
    "          \"London\":'UK',\n",
    "          \"Los Angeles, CA\":'USA',\n",
    "          \"Washington, D.C.\":'USA',\n",
    "          \"California\":'USA',\n",
    "          \"Chicago, IL\":'USA',\n",
    "          \"Chicago\":'USA',\n",
    "          \"New York, NY\":'USA',\n",
    "          \"California, USA\":'USA',\n",
    "          \"FLorida\":'USA',\n",
    "          \"Nigeria\":'Africa',\n",
    "          \"Kenya\":'Africa',\n",
    "          \"Everywhere\":'Worldwide',\n",
    "          \"San Francisco\":'USA',\n",
    "          \"Florida\":'USA',\n",
    "          \"United Kingdom\":'UK',\n",
    "          \"Los Angeles\":'USA',\n",
    "          \"Toronto\":'Canada',\n",
    "          \"San Francisco, CA\":'USA',\n",
    "          \"NYC\":'USA',\n",
    "          \"Seattle\":'USA',\n",
    "          \"Earth\":'Worldwide',\n",
    "          \"Ireland\":'UK',\n",
    "          \"London, England\":'UK',\n",
    "          \"New York City\":'USA',\n",
    "          \"Texas\":'USA',\n",
    "          \"London, UK\":'UK',\n",
    "          \"Atlanta, GA\":'USA',\n",
    "          \"England, United Kingdom\":'UK',\n",
    "          \"Mumbai, India\":'India',\n",
    "          \"Melbourne,Victoria\":'Australia'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e5347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply a function and change the name of the location to group if it exists on the mapping key\n",
    "df['location'] = df['location'].apply(lambda i: mapping[i] if i in mapping.keys() else i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a43d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df\n",
    "df.to_csv('df_transform.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bc3e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the values changed\n",
    "ndf = df\n",
    "ndf.dropna()\n",
    "location = ndf['location'].value_counts()\n",
    "location[location>=10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2319304",
   "metadata": {},
   "outputs": [],
   "source": [
    "localizator = Nominatim(user_agent='tweets-analysis') # Creation of the agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04240d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = list(location.index) #We keep just the name of the cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-notification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geolocated = list(map(lambda x: [x,localizator.geocode(x,timeout=None)[1] if localizator.geocode(x,timeout=None) else None],location2))\n",
    "# geolocated = pd.DataFrame(geolocated)\n",
    "# geolocated.columns = ['locat','latlong']\n",
    "# try:\n",
    "#   geolocated['lat'] = geolocated.latlong.apply(lambda x: x[0])\n",
    "#   geolocated['lon'] = geolocated.latlong.apply(lambda x: x[1])\n",
    "#   geolocated.drop('latlong',axis=1, inplace=True)\n",
    "# except:\n",
    "#   pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b21094",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocated = pd.read_csv('coords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd726fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocated.drop(['Unnamed: 0','Unnamed: 0.1'],1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e84052",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocated['latlong']=geolocated.latlong.apply(lambda x: x[1:-1].split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a835e11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "  geolocated['lat'] = geolocated.latlong.apply(lambda x: x[0])\n",
    "  geolocated['lon'] = geolocated.latlong.apply(lambda x: x[1])\n",
    "  geolocated.drop('latlong',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad8775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocated['lat']=pd.to_numeric(geolocated['lat'])\n",
    "geolocated['lon']=pd.to_numeric(geolocated['lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30607688",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_map = gpd.read_file('World_Countries.shp')\n",
    "fig,ax = plt.subplots(figsize=(15,15))\n",
    "world_map.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32faf7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crs = {'init':'epsg:4326'}\n",
    "geometry = [Point(xy) for xy in zip(geolocated['lon'],geolocated['lat'])]\n",
    "geo_df = gpd.GeoDataFrame(geolocated,crs=crs,geometry=geometry)\n",
    "geo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(25,25))\n",
    "ax.set(facecolor='Powderblue')\n",
    "ax.set_alpha(0.2)\n",
    "world_map.plot(ax=ax, alpha=1, color='white')\n",
    "geo_df.plot(ax=ax,markersize=30,color='darkviolet',marker='o',alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2922e846",
   "metadata": {},
   "source": [
    "# Word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "944e1cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7325e14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['font.size'] = 15\n",
    "mpl.rcParams['savefig.dpi'] = 100\n",
    "mpl.rcParams['figure.subplot.bottom'] = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "26e23ea5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-0589c52b57c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTOPWORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m wordcloud = WordCloud(\n\u001b[0m\u001b[1;32m      4\u001b[0m                         \u001b[0mbackground_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'white'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         \u001b[0mstopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Tweet_Disasters/venv/lib/python3.9/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \"\"\"\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Tweet_Disasters/venv/lib/python3.9/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \"\"\"\n\u001b[0;32m--> 613\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Tweet_Disasters/venv/lib/python3.9/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mprocess_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0mregexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m         \u001b[0;31m# remove 's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         words = [word[:-2] if word.lower().endswith(\"'s\") else word\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/re.py\u001b[0m in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     Empty matches are included in the result.\"\"\"\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "                        background_color='white',\n",
    "                        stopwords=stopwords,\n",
    "                        max_words = 200,\n",
    "                        max_font_size = 40,\n",
    "                        random_state = 42,\n",
    "                        ).generate(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baac7493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec51825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c724cfc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13c8570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ead75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b773ca6",
   "metadata": {},
   "source": [
    "# Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cf37b46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Communal violence in Bhainsa, Telangana. \"Ston...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Telangana: Section 144 has been imposed in Bha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>New York City</td>\n",
       "      <td>Arsonist sets cars ablaze at dealership https:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Morgantown, WV</td>\n",
       "      <td>Arsonist sets cars ablaze at dealership https:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Lord Jesus, your love brings freedom and pard...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11365</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>Blue State in a red sea</td>\n",
       "      <td>Media should have warned us well in advance. T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11366</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>arohaonces</td>\n",
       "      <td>i feel directly attacked ðŸ’€ i consider moonbin ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11367</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>ðŸ‡µðŸ‡­</td>\n",
       "      <td>i feel directly attacked ðŸ’€ i consider moonbin ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11368</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>auroraborealis</td>\n",
       "      <td>ok who remember \"outcast\" nd the \"dora\" au?? T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11369</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jake Corway wrecked while running 14th at IRP.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11370 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       keyword                 location  \\\n",
       "id                                        \n",
       "0       ablaze                      NaN   \n",
       "1       ablaze                      NaN   \n",
       "2       ablaze            New York City   \n",
       "3       ablaze           Morgantown, WV   \n",
       "4       ablaze                      NaN   \n",
       "...        ...                      ...   \n",
       "11365  wrecked  Blue State in a red sea   \n",
       "11366  wrecked               arohaonces   \n",
       "11367  wrecked                       ðŸ‡µðŸ‡­   \n",
       "11368  wrecked           auroraborealis   \n",
       "11369  wrecked                      NaN   \n",
       "\n",
       "                                                    text  target  \n",
       "id                                                                \n",
       "0      Communal violence in Bhainsa, Telangana. \"Ston...       1  \n",
       "1      Telangana: Section 144 has been imposed in Bha...       1  \n",
       "2      Arsonist sets cars ablaze at dealership https:...       1  \n",
       "3      Arsonist sets cars ablaze at dealership https:...       1  \n",
       "4      \"Lord Jesus, your love brings freedom and pard...       0  \n",
       "...                                                  ...     ...  \n",
       "11365  Media should have warned us well in advance. T...       0  \n",
       "11366  i feel directly attacked ðŸ’€ i consider moonbin ...       0  \n",
       "11367  i feel directly attacked ðŸ’€ i consider moonbin ...       0  \n",
       "11368  ok who remember \"outcast\" nd the \"dora\" au?? T...       0  \n",
       "11369     Jake Corway wrecked while running 14th at IRP.       1  \n",
       "\n",
       "[11370 rows x 4 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd5d0bf0",
   "metadata": {},
   "source": [
    "## We have more targets with 0 that means no disaster and less targets 1 meaning disaster\n",
    "This could affect the resutls and accuracy in the trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e3cabee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='target'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = df['target'].value_counts().index\n",
    "y = df['target'].value_counts()\n",
    "\n",
    "sns.barplot(x=x ,y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c223cb8",
   "metadata": {},
   "source": [
    "### We have a look on how does a Distaster and No Disaster Tweet looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "417c3313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Asansol: A BJP office in Salanpur village was set ablaze last night. BJP has alleged that TMC is behind the incident. Police has bâ€¦'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we see how a Disaster tweet looks like (Target 1)\n",
    "disaster_tweets=df[df['target']==1]['text']\n",
    "disaster_tweets.values[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67704efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Warm greetings to all on the occasion of #Lohri. As winter passes by may everyone's woes and troubles be set ablaze in tâ€¦\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we see how a non Disaster tweet looks like (Target 0)\n",
    "nondisaster_tweets=df[df['target']==0]['text']\n",
    "nondisaster_tweets.values[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b24b532f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    communal violence in bhainsa telangana stones ...\n",
       "1    telangana section  has been imposed in bhainsa...\n",
       "2             arsonist sets cars ablaze at dealership \n",
       "3            arsonist sets cars ablaze at dealership  \n",
       "4    lord jesus your love brings freedom and pardon...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Esta celda se va a quitar cuando estÃ© todo el cÃ³digo, estoy limpiando aqui para hacer el ejercicio\n",
    "# Applying a first round of text cleaning techniques\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "# Applying the cleaning function to both test and training datasets\n",
    "df['text'] = df['text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "# Let's take a look at the updated text\n",
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cfb0ef",
   "metadata": {},
   "source": [
    "### Tokenizing \n",
    "Now we have to tokenize each Tweet, this mean that we have to split each sentence in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebf7a58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [communal, violence, bhainsa, telangana, stone...\n",
       "1    [telangana, section, imposed, bhainsa, january...\n",
       "2           [arsonist, sets, cars, ablaze, dealership]\n",
       "3           [arsonist, sets, cars, ablaze, dealership]\n",
       "4    [lord, jesus, love, brings, freedom, pardon, f...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize the dataset\n",
    "tokenizer=nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "df['text']=df['text'].apply(lambda x:tokenizer.tokenize(x))\n",
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408de0da",
   "metadata": {},
   "source": [
    "###  Removing Stopwords\n",
    "Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like, the, he, have etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52f2e5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[communal, violence, bhainsa, telangana, stone...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[telangana, section, imposed, bhainsa, january...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>New York City</td>\n",
       "      <td>[arsonist, sets, cars, ablaze, dealership]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Morgantown, WV</td>\n",
       "      <td>[arsonist, sets, cars, ablaze, dealership]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[lord, jesus, love, brings, freedom, pardon, f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword        location  \\\n",
       "0   0  ablaze             NaN   \n",
       "1   1  ablaze             NaN   \n",
       "2   2  ablaze   New York City   \n",
       "3   3  ablaze  Morgantown, WV   \n",
       "4   4  ablaze             NaN   \n",
       "\n",
       "                                                text  target  \n",
       "0  [communal, violence, bhainsa, telangana, stone...       1  \n",
       "1  [telangana, section, imposed, bhainsa, january...       1  \n",
       "2         [arsonist, sets, cars, ablaze, dealership]       1  \n",
       "3         [arsonist, sets, cars, ablaze, dealership]       1  \n",
       "4  [lord, jesus, love, brings, freedom, pardon, f...       0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Removing stopwords belonging to english language\n",
    "    \n",
    "    \"\"\"\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x : remove_stopwords(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d487fe7b",
   "metadata": {},
   "source": [
    "### After the Stopwords we need to re join the whole text in to one string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a367e295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>communal violence bhainsa telangana stones pel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>telangana section imposed bhainsa january clas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>New York City</td>\n",
       "      <td>arsonist sets cars ablaze dealership</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Morgantown, WV</td>\n",
       "      <td>arsonist sets cars ablaze dealership</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lord jesus love brings freedom pardon fill hol...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword        location  \\\n",
       "0   0  ablaze             NaN   \n",
       "1   1  ablaze             NaN   \n",
       "2   2  ablaze   New York City   \n",
       "3   3  ablaze  Morgantown, WV   \n",
       "4   4  ablaze             NaN   \n",
       "\n",
       "                                                text  target  \n",
       "0  communal violence bhainsa telangana stones pel...       1  \n",
       "1  telangana section imposed bhainsa january clas...       1  \n",
       "2               arsonist sets cars ablaze dealership       1  \n",
       "3               arsonist sets cars ablaze dealership       1  \n",
       "4  lord jesus love brings freedom pardon fill hol...       0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After preprocessing, the text format\n",
    "def combine_text(list_of_text):\n",
    "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ' '.join(list_of_text)\n",
    "    return combined_text\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x : combine_text(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bf379d",
   "metadata": {},
   "source": [
    "### Spliting the data to train the model\n",
    "We need to train our model with the same data set, but we need to assign in this case 80% to train and 20% of the same data to test it. This way we can see if the accuracy of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0525fa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into independent and dependent features\n",
    "X=df['text']\n",
    "y=df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4ffd7091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    communal violence bhainsa telangana stones pel...\n",
       "1    telangana section imposed bhainsa january clas...\n",
       "2                 arsonist sets cars ablaze dealership\n",
       "3                 arsonist sets cars ablaze dealership\n",
       "4    lord jesus love brings freedom pardon fill hol...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682b5de0",
   "metadata": {},
   "source": [
    "### Here the ```test_size=0.2``` means that we are going to take the 20% of the data to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a884ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need data to train the model and data to test it. In this case we have 20% data to test\n",
    "X_train,X_test,y_train,y_test = model_selection.train_test_split(X,y,test_size=0.2,random_state=1)\n",
    "\n",
    "#Convert a collection of text documents to a matrix of token counts\n",
    "vectorizer=CountVectorizer()\n",
    "x_train_vectors=vectorizer.fit_transform(X_train)\n",
    "x_test_vectors=vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f015d683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9981    breaking news us attorney general william barr...\n",
       "8767    photos taken iraq base us troops stationed sho...\n",
       "6900    ordinarily player would expected return presea...\n",
       "7068    countries treat taiwan partner issue reelected...\n",
       "8380    god storms patron house since left earth im go...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "01a2a94d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have a matrix with all the words converted to numbers\n",
    "x_train_vectors.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6ed131",
   "metadata": {},
   "source": [
    "### Now we can see the metrics of our model\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "256f42a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score train: 0.9478891820580475\n",
      "Accuracy score test 0.8698328935795955\n",
      "__________________________________________________\n",
      "\n",
      "-Classification report train:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.97      7395\n",
      "           1       0.80      0.97      0.87      1701\n",
      "\n",
      "    accuracy                           0.95      9096\n",
      "   macro avg       0.89      0.95      0.92      9096\n",
      "weighted avg       0.96      0.95      0.95      9096\n",
      "\n",
      "-Classification report test:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.89      0.92      1861\n",
      "           1       0.61      0.77      0.68       413\n",
      "\n",
      "    accuracy                           0.87      2274\n",
      "   macro avg       0.78      0.83      0.80      2274\n",
      "weighted avg       0.89      0.87      0.88      2274\n",
      "\n",
      "Compute Area Under the Receiver Operating Characteristic Curve  train: 0.9545977607731045\n",
      "Compute Area Under the Receiver Operating Characteristic Curve test: 0.8309846693893908\n",
      "__________________________________________________\n",
      "\n",
      "Confusion Matrix Train:\n",
      " [[6980  415]\n",
      " [  59 1642]]\n",
      "Confusion Matrix Test:\n",
      " [[1660  201]\n",
      " [  95  318]]\n"
     ]
    }
   ],
   "source": [
    "clf_naive=MultinomialNB(alpha=0.2,fit_prior=False)\n",
    "clf_naive.fit(x_train_vectors,y_train)\n",
    "pred=clf_naive.predict(x_test_vectors)\n",
    "\n",
    "accuracy_score_train=metrics.accuracy_score(y_train,clf_naive.predict(x_train_vectors))\n",
    "print(f'Accuracy score train: {accuracy_score_train}')\n",
    "\n",
    "accuracy_score_test=metrics.accuracy_score(y_test,pred)\n",
    "print(f'Accuracy score test {accuracy_score_test}')\n",
    "print('_____'*10+'\\n')\n",
    "\n",
    "classification_report_train=metrics.classification_report(y_train,clf_naive.predict(x_train_vectors))\n",
    "print(f'-Classification report train:\\n {classification_report_train}')\n",
    "\n",
    "classification_report_test=metrics.classification_report(y_test,pred)\n",
    "print(f'-Classification report test:\\n {classification_report_test}')\n",
    "\n",
    "roc_auc_score_train=metrics.roc_auc_score(y_train,clf_naive.predict(x_train_vectors))\n",
    "print(f'Compute Area Under the Receiver Operating Characteristic Curve  train: {roc_auc_score_train}')\n",
    "\n",
    "roc_auc_score_test=metrics.roc_auc_score(y_test,pred)\n",
    "print(f'Compute Area Under the Receiver Operating Characteristic Curve test: {roc_auc_score_test}')\n",
    "\n",
    "print('_____'*10+'\\n')\n",
    "confusion_matrix_train=metrics.confusion_matrix(y_train,clf_naive.predict(x_train_vectors))\n",
    "print(f'Confusion Matrix Train:\\n {confusion_matrix_train}')\n",
    "\n",
    "confusion_matrix_test=metrics.confusion_matrix(y_test,pred)\n",
    "print(f'Confusion Matrix Test:\\n {confusion_matrix_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa59797",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5fe2c4",
   "metadata": {},
   "source": [
    "At the beginning the train and test score seems to be near to 1 with a good accuracy. Then on the classification report we can see that the precision is overfitted on the class '0' and if we see the support column there is a huge difference between bot targets, we have 7935 tweets with 0 and 1701 with 1. This mean that the model data is unbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59974c06",
   "metadata": {},
   "source": [
    "https://muthu.co/understanding-the-classification-report-in-sklearn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c348ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
