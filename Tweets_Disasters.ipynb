{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b7b516",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "#Import scikit-learn dataset library\n",
    "import pandas as pd\n",
    "#Text processing libraries\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords') \n",
    "#sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "#Libraries for plotting\n",
    "import seaborn as sns\n",
    "#Modules for plotting\n",
    "from matplotlib import pyplot as plt\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point , Polygon\n",
    "import descartes\n",
    "#Import Nominatim for transform city names\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "df = pd.read_csv('tweets.csv').set_index('id')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a49897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying a first round of text cleaning techniques to the text column\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "# Applying the cleaning function\n",
    "df['text'] = df['text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: deEmojify(x))\n",
    "# I'm going to comment on this line that I think is the one who return an error\n",
    "#df.convert_dtypes['location'] = df['location'].apply(lambda x: deEmojify(x))\n",
    "\n",
    "# Let's take a look at the updated text\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e52ea",
   "metadata": {},
   "source": [
    "## Cleaning the 'Keyword' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b486344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we're going to check this column\n",
    "df.keyword.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70454bb",
   "metadata": {},
   "source": [
    "As we can see there are 219 unique values in this column, let's see what are those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adadc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the unique values in the 'keyword' column\n",
    "keywords = df['keyword'].unique()\n",
    "\n",
    "# sort them alphabetically and then take a closer look\n",
    "keywords.sort()\n",
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bea171c",
   "metadata": {},
   "source": [
    "As we can see the only characters that are unusual are \"%20\" ones, so we can replace them for an empty space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed87a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the \"%20\" from the registers in the column 'keyword'\n",
    "df['keyword'] = df['keyword'].apply(lambda x: x.replace(\"%20\",\" \"))\n",
    "\n",
    "#Change the dtype of the column from object to str\n",
    "df.keyword = df.keyword.astype(str)\n",
    "\n",
    "# get all the unique values in the 'keyword' column\n",
    "keywords = df['keyword'].unique()\n",
    "\n",
    "# sort them alphabetically and then take a closer look\n",
    "keywords.sort()\n",
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66161628",
   "metadata": {},
   "source": [
    "As we can see, the unique values still remain 219, but we successfully removed all the '%20' characters from our registers in the 'keyword' column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113ec1f5",
   "metadata": {},
   "source": [
    "# Transform the emoji flag to a string in the 'location' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b506c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply quotation marks to all registers in the \"location\" column (this will allowed us to treat the emoji flags later with the\n",
    "# emoji library).\n",
    "df.location = df.location.astype(str)\n",
    "df.update(df[['location']].applymap('\\'{}\\''.format))\n",
    "\n",
    "# Let's take a look at the updated text\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a0b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the emoji flags in the location column \n",
    "def emojiflag_to_text(flag):\n",
    "    '''Convert the emoji flag to a string with their name. the emoji flags are two unicode letters that composes \n",
    "    for example ðŸ‡²ðŸ‡½ -> MX, so we know that the lenght of an emoji flag is 2, as we put it in a simple quotation marks, \n",
    "    we can assume that an emoji flag has lenght equal to 4'''\n",
    "    if len(flag) == 4:\n",
    "      return emoji.demojize(flag)\n",
    "    else:\n",
    "      return flag\n",
    "     \n",
    "\n",
    "\n",
    "# Applying the transform function\n",
    "df['location'] = df['location'].apply(lambda x: emojiflag_to_text(x))\n",
    "\n",
    "# Let's take a look at the updated text\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0eb31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the simple quotes from all the registers in the \"location\" column.\n",
    "df['location'] = df['location'].apply(lambda x: x.replace(\"'\",\"\"))\n",
    "\n",
    "#Remove the \":\" from the registers that used to be an emoji flag\n",
    "df['location'] = df['location'].apply(lambda x: x.replace(\":\",\"\"))\n",
    "\n",
    "#Remove the strings nan with a true NaN\n",
    "df['location'] = df['location'].apply(lambda x: x.replace(\"nan\",\"\"))\n",
    "\n",
    "#Remove all the emojis (no emoji flags) that remains in the 'location' column\n",
    "df['location'] = df['location'].str.replace('[^\\w\\s#@/:%.,_-]', '', flags=re.UNICODE)\n",
    "\n",
    "# Let's take a look at the updated text\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe54aea",
   "metadata": {},
   "source": [
    "# Taking note of dataset issues\n",
    "For the dataset Tweets we can see that only the column called location has missing values. We only have **7,952** locations of **11,370** Tweets.\n",
    "And some emojis in text that we have to clean later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8154578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "print('\\n')\n",
    "print('Data Type')\n",
    "print('__'*12)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08746ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "nul_locations = df['location'].isna().sum()\n",
    "print(f'We have {nul_locations} missing locations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e329fd",
   "metadata": {},
   "source": [
    "In order to get the best data type to work with we can use `.convert_dtypes()` . To change the data type and have a better workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5de32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data Type Converted')\n",
    "print('__'*12)\n",
    "df.convert_dtypes().dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ea1dd9",
   "metadata": {},
   "source": [
    "## Here we have another look.\n",
    "*   219 unique keywords that describes the tweets\n",
    "*   4,504 unique locations wich need to be grouped later on\n",
    "*   11,223 Tweets (text) wich make us think if there is a kind of   duplicated message. \n",
    "*   And 2 different variables in target, 0 and 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0858ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use unique to know the different variables that conforms each column\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e184c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if there is any duplicated text\n",
    "dup_text = df['text'].duplicated().sum()\n",
    "print(f'We have {dup_text} duplicated texts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f96a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can have a description of the lenght of the text, the largest and shortest text and maybe a relation with the target number. \n",
    "df['length'] = [len(i) for i in df['text']]\n",
    "df['length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e44eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram with the distribution of the Tweets\n",
    "plt.hist(df.length)\n",
    "plt.xlabel('Length of Tweet')\n",
    "plt.ylabel('# of Tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533b2913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table with the highest and lowest lengths\n",
    "display(df[df['length']>130])\n",
    "print('\\n')\n",
    "display(df[df['length']<20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd18f78",
   "metadata": {},
   "source": [
    "# Where do the Tweets come from?\n",
    "We take a first look to the availables places that we have. We are getting the locations that occurs at least 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02278c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = df['location'].value_counts()\n",
    "location[location>=10][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e4453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to group some places, so we create a mapping dictionary to replace the locations\n",
    "mapping = {'United States':'USA',\n",
    "          'New York':'USA',\n",
    "          \"London\":'UK',\n",
    "          \"Los Angeles, CA\":'USA',\n",
    "          \"Washington, D.C.\":'USA',\n",
    "          \"California\":'USA',\n",
    "          \"Chicago, IL\":'USA',\n",
    "          \"Chicago\":'USA',\n",
    "          \"New York, NY\":'USA',\n",
    "          \"California, USA\":'USA',\n",
    "          \"FLorida\":'USA',\n",
    "          \"Nigeria\":'Africa',\n",
    "          \"Kenya\":'Africa',\n",
    "          \"Everywhere\":'Worldwide',\n",
    "          \"San Francisco\":'USA',\n",
    "          \"Florida\":'USA',\n",
    "          \"United Kingdom\":'UK',\n",
    "          \"Los Angeles\":'USA',\n",
    "          \"Toronto\":'Canada',\n",
    "          \"San Francisco, CA\":'USA',\n",
    "          \"NYC\":'USA',\n",
    "          \"Seattle\":'USA',\n",
    "          \"Earth\":'Worldwide',\n",
    "          \"Ireland\":'UK',\n",
    "          \"London, England\":'UK',\n",
    "          \"New York City\":'USA',\n",
    "          \"Texas\":'USA',\n",
    "          \"London, UK\":'UK',\n",
    "          \"Atlanta, GA\":'USA',\n",
    "          \"England, United Kingdom\":'UK',\n",
    "          \"Mumbai, India\":'India',\n",
    "          \"Melbourne,Victoria\":'Australia'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e5347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply a function and change the name of the location to group if it exists on the mapping key\n",
    "df['location'] = df['location'].apply(lambda i: mapping[i] if i in mapping.keys() else i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a43d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df\n",
    "df.to_csv('df_transform.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bc3e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the values changed\n",
    "ndf = df\n",
    "ndf.dropna()\n",
    "location = ndf['location'].value_counts()\n",
    "location[location>=10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2319304",
   "metadata": {},
   "outputs": [],
   "source": [
    "localizator = Nominatim(user_agent='tweets-analysis') # Creation of the agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04240d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = list(location.index) #We keep just the name of the cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-notification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geolocated = list(map(lambda x: [x,localizator.geocode(x,timeout=None)[1] if localizator.geocode(x,timeout=None) else None],location2))\n",
    "# geolocated = pd.DataFrame(geolocated)\n",
    "# geolocated.columns = ['locat','latlong']\n",
    "# try:\n",
    "#   geolocated['lat'] = geolocated.latlong.apply(lambda x: x[0])\n",
    "#   geolocated['lon'] = geolocated.latlong.apply(lambda x: x[1])\n",
    "#   geolocated.drop('latlong',axis=1, inplace=True)\n",
    "# except:\n",
    "#   pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b21094",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocated = pd.read_csv('coords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd726fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocated.drop(['Unnamed: 0','Unnamed: 0.1'],1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e84052",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocated['latlong']=geolocated.latlong.apply(lambda x: x[1:-1].split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a835e11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "  geolocated['lat'] = geolocated.latlong.apply(lambda x: x[0])\n",
    "  geolocated['lon'] = geolocated.latlong.apply(lambda x: x[1])\n",
    "  geolocated.drop('latlong',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad8775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocated['lat']=pd.to_numeric(geolocated['lat'])\n",
    "geolocated['lon']=pd.to_numeric(geolocated['lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30607688",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_map = gpd.read_file('World_Countries.shp')\n",
    "fig,ax = plt.subplots(figsize=(15,15))\n",
    "world_map.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32faf7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crs = {'init':'epsg:4326'}\n",
    "geometry = [Point(xy) for xy in zip(geolocated['lon'],geolocated['lat'])]\n",
    "geo_df = gpd.GeoDataFrame(geolocated,crs=crs,geometry=geometry)\n",
    "geo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(25,25))\n",
    "ax.set(facecolor='Powderblue')\n",
    "ax.set_alpha(0.2)\n",
    "world_map.plot(ax=ax, alpha=1, color='white')\n",
    "geo_df.plot(ax=ax,markersize=30,color='darkviolet',marker='o',alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2922e846",
   "metadata": {},
   "source": [
    "# Word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944e1cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7325e14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['font.size'] = 15\n",
    "mpl.rcParams['savefig.dpi'] = 100\n",
    "mpl.rcParams['figure.subplot.bottom'] = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e23ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "                        background_color='white',\n",
    "                        stopwords=stopwords,\n",
    "                        max_words = 200,\n",
    "                        max_font_size = 40,\n",
    "                        random_state = 42,\n",
    "                        ).generate(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baac7493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec51825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c724cfc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13c8570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ead75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b773ca6",
   "metadata": {},
   "source": [
    "# Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf37b46e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd5d0bf0",
   "metadata": {},
   "source": [
    "## We have more targets with 0 that means no disaster and less targets 1 meaning disaster\n",
    "This could affect the resutls and accuracy in the trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3cabee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['target'].value_counts().index\n",
    "y = df['target'].value_counts()\n",
    "\n",
    "sns.barplot(x=x ,y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c223cb8",
   "metadata": {},
   "source": [
    "### We have a look on how does a Distaster and No Disaster Tweet looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417c3313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we see how a Disaster tweet looks like (Target 1)\n",
    "disaster_tweets=df[df['target']==1]['text']\n",
    "disaster_tweets.values[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67704efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we see how a non Disaster tweet looks like (Target 0)\n",
    "nondisaster_tweets=df[df['target']==0]['text']\n",
    "nondisaster_tweets.values[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24b532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Esta celda se va a quitar cuando estÃ© todo el cÃ³digo, estoy limpiando aqui para hacer el ejercicio\n",
    "# Applying a first round of text cleaning techniques\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "# Applying the cleaning function to both test and training datasets\n",
    "df['text'] = df['text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "# Let's take a look at the updated text\n",
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cfb0ef",
   "metadata": {},
   "source": [
    "### Tokenizing \n",
    "Now we have to tokenize each Tweet, this mean that we have to split each sentence in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf7a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the dataset\n",
    "tokenizer=nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "df['text']=df['text'].apply(lambda x:tokenizer.tokenize(x))\n",
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408de0da",
   "metadata": {},
   "source": [
    "###  Removing Stopwords\n",
    "Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like, the, he, have etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f2e5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Removing stopwords belonging to english language\n",
    "    \n",
    "    \"\"\"\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x : remove_stopwords(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d487fe7b",
   "metadata": {},
   "source": [
    "### After the Stopwords we need to re join the whole text in to one string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a367e295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After preprocessing, the text format\n",
    "def combine_text(list_of_text):\n",
    "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ' '.join(list_of_text)\n",
    "    return combined_text\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x : combine_text(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bf379d",
   "metadata": {},
   "source": [
    "### Spliting the data to train the model\n",
    "We need to train our model with the same data set, but we need to assign in this case 80% to train and 20% of the same data to test it. This way we can see if the accuracy of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0525fa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into independent and dependent features\n",
    "X=df['text']\n",
    "y=df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffd7091",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682b5de0",
   "metadata": {},
   "source": [
    "### Here the ```test_size=0.2``` means that we are going to take the 20% of the data to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a884ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need data to train the model and data to test it. In this case we have 20% data to test\n",
    "X_train,X_test,y_train,y_test = model_selection.train_test_split(X,y,test_size=0.2,random_state=1)\n",
    "\n",
    "#Convert a collection of text documents to a matrix of token counts\n",
    "vectorizer=CountVectorizer()\n",
    "x_train_vectors=vectorizer.fit_transform(X_train)\n",
    "x_test_vectors=vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f015d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a2a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a matrix with all the words converted to numbers\n",
    "x_train_vectors.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6ed131",
   "metadata": {},
   "source": [
    "### Now we can see the metrics of our model\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256f42a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_naive=MultinomialNB(alpha=0.2,fit_prior=False)\n",
    "clf_naive.fit(x_train_vectors,y_train)\n",
    "pred=clf_naive.predict(x_test_vectors)\n",
    "\n",
    "accuracy_score_train=metrics.accuracy_score(y_train,clf_naive.predict(x_train_vectors))\n",
    "print(f'Accuracy score train: {accuracy_score_train}')\n",
    "\n",
    "accuracy_score_test=metrics.accuracy_score(y_test,pred)\n",
    "print(f'Accuracy score test {accuracy_score_test}')\n",
    "print('_____'*10+'\\n')\n",
    "\n",
    "classification_report_train=metrics.classification_report(y_train,clf_naive.predict(x_train_vectors))\n",
    "print(f'-Classification report train:\\n {classification_report_train}')\n",
    "\n",
    "classification_report_test=metrics.classification_report(y_test,pred)\n",
    "print(f'-Classification report test:\\n {classification_report_test}')\n",
    "\n",
    "roc_auc_score_train=metrics.roc_auc_score(y_train,clf_naive.predict(x_train_vectors))\n",
    "print(f'Compute Area Under the Receiver Operating Characteristic Curve  train: {roc_auc_score_train}')\n",
    "\n",
    "roc_auc_score_test=metrics.roc_auc_score(y_test,pred)\n",
    "print(f'Compute Area Under the Receiver Operating Characteristic Curve test: {roc_auc_score_test}')\n",
    "\n",
    "print('_____'*10+'\\n')\n",
    "confusion_matrix_train=metrics.confusion_matrix(y_train,clf_naive.predict(x_train_vectors))\n",
    "print(f'Confusion Matrix Train:\\n {confusion_matrix_train}')\n",
    "\n",
    "confusion_matrix_test=metrics.confusion_matrix(y_test,pred)\n",
    "print(f'Confusion Matrix Test:\\n {confusion_matrix_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa59797",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5fe2c4",
   "metadata": {},
   "source": [
    "At the beginning the train and test score seems to be near to 1 with a good accuracy. Then on the classification report we can see that the precision is overfitted on the class '0' and if we see the support column there is a huge difference between bot targets, we have 7935 tweets with 0 and 1701 with 1. This mean that the model data is unbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59974c06",
   "metadata": {},
   "source": [
    "https://muthu.co/understanding-the-classification-report-in-sklearn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c348ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
